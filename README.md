



# NLP Adversarial Toolkit: A Reimplementation of Advanced NLP Attack Methods

## Description
This repository is a reimplementation of the key concepts from "A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP". It aims to explore and enhance NLP models through adversarial methods. The project encompasses adversarial attack techniques, data augmentation methods, and adversarial training approaches.

## Getting Started

### Cloning the TextAttack Library
To use the TextAttack library, first clone the repository:

```
git clone https://github.com/QData/TextAttack.git
```

### Installation
Install the package in editable mode for development purposes:

```
pip install -e .
```

### Command-Line Training
Train an LSTM model on the SST-2 dataset for 8 epochs:

```
!textattack train --model-name-or-path lstm --dataset sst2 --model-num-labels 2 --per-device-train-batch-size 128 --num-epochs 8 --learning-rate 2e-5 --output-dir "/content/drive/MyDrive/ai/e8/"
```
For shorter training runs:

```
!textattack train --model-name-or-path lstm --dataset sst2 --model-num-labels 2 --per-device-train-batch-size 128 --num-epochs 3 --learning-rate 2e-5 --output-dir "/"
```

### Adversarial Training with TextFooler
Train with adversarial examples generated by TextFooler:

```
!textattack train --model-name-or-path lstm --attack-epoch-interval 7 --num-epochs 7 --num-train-adv-examples 1000 --attack textfooler --dataset sst2 --per-device-train-batch-size 128 --model-num-labels 2 --output-dir /content/drive/MyDrive/ai/adv/
```

### Adding More Constraints
Run an attack with the Pruthi recipe and additional constraints:

```
!textattack attack --recipe pruthi --num-examples 5000 --transformation WordSwapQWERTY --model-batch-size 128 --constraints "max-words-perturbed^max_num word=1" "max-words-perturbed^compare_against_original=True" --model /content/drive/MyDrive/ai/earlytrainning/best_model/ --dataset-from-huggingface sst2
```

### Programmatically Training a WordCNN Model
Train a WordCNN model with augmented data:

```python
# Assuming 'augmented_data' contains your augmented dataset
ag = Dataset(augmented_data)
print("Length of augmented dataset:", len(ag))

model = WordCNNForClassification(num_labels=2)
tokenizer = model.tokenizer

# Wrap the model with PyTorchModelWrapper
model_wrapper = PyTorchModelWrapper(model, tokenizer)

# Define the training arguments
training_args = TrainingArgs(
    num_epochs=3,
    per_device_train_batch_size=128
)

# Initialize and configure the Trainer
trainer = Trainer(
    model_wrapper,
    "classification",
    None,
    ag,
    test_dataset,
    training_args
)

# Start the training process
trainer.train()
```

### Data Augmentation with EDA
Augment your dataset using EasyDataAugmenter:

```python
eda_augmenter = EasyDataAugmenter(pct_words_to_swap=0.1, transformations_per_example=4)
augmented_data = [(eda_augmenter.augment(d[0])[0], d[1]) for d in train_dataset]
```

### Setting Up Models for Training

When working with different model classes in TextAttack, especially if you are switching between LSTM and CNN models or between PyTorch and TensorFlow frameworks, you might need to adjust your setup accordingly. Below is an example of how to set up models depending on their class:

```python
if model_class == "LSTMForClassification":
    model = textattack.models.helpers.LSTMForClassification.from_pretrained(args.model)
    model = textattack.models.wrappers.PyTorchModelWrapper(model, model.tokenizer)
elif model_class == "WordCNNForClassification":
    model = textattack.models.helpers.WordCNNForClassification.from_pretrained(args.model)
    model = textattack.models.wrappers.PyTorchModelWrapper(model, model.tokenizer)

